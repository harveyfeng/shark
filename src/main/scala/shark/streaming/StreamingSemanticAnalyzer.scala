package shark.streaming

import scala.collection.JavaConversions._
import scala.collection.mutable.{ArrayBuffer, ListBuffer}

import java.lang.reflect.Method
import java.util.{ArrayList, List => JavaList, Map => JavaMap}

import org.apache.hadoop.fs.Path
import org.apache.hadoop.hive.conf.HiveConf
import org.apache.hadoop.hive.metastore.api.{FieldSchema, MetaException}
import org.apache.hadoop.hive.metastore.Warehouse
import org.apache.hadoop.hive.ql.exec.{DDLTask, FetchTask, MoveTask, Task, TaskFactory}
import org.apache.hadoop.hive.ql.metadata.HiveException
import org.apache.hadoop.hive.ql.optimizer.Optimizer
import org.apache.hadoop.hive.ql.parse._
import org.apache.hadoop.hive.ql.plan._
import org.apache.hadoop.hive.ql.session.SessionState

import shark.execution.{HiveOperator, Operator, SparkTask, TableScanOperator, TerminalOperator}
import shark.execution.TableRDD
import shark.parse.{SharkSemanticAnalyzer, QueryContext}
import shark.SharkEnv

import spark.streaming.{DStream, Duration, StreamingContext}
import spark.RDD


class StreamingSemanticAnalyzer(conf: HiveConf) extends SharkSemanticAnalyzer(conf) {

  override def analyzeInternal(ast: ASTNode): Unit = {
    reset()

    val qb = new QB(null, null, false)
    var pctx = getParseContext()
    pctx.setQB(qb)
    pctx.setParseTree(ast)
    init(pctx)

    val cmdContext = pctx.getContext().asInstanceOf[StreamingCommandContext]
    var isCTAS = false

    logInfo("Starting Shark Streaming Semantic Analysis")

    // if (ast.getToken().getType() == SharkParser.TOK_CREATESTREAM) isCreateStream = true
    // Analyze CREATE TABLE command
    if (ast.getToken().getType() == HiveParser.TOK_CREATETABLE) {
      // Note: this means streams are tables...
      // super.analyzeInternal(ast)
      for (ch <- ast.getChildren.asInstanceOf[JavaList[ASTNode]]) {
        ch.getToken.getType match {
          case HiveParser.TOK_QUERY => {
            // Fill cmdContext with metadata info, such as stream <-> window mapping.
            ASTTraversal.processQueryNode(ch, cmdContext)
            isCTAS = true
          }
          case _ =>
            Unit
        }
      }

      // TODO: temporary
      // if streaming, get data needed to create DStreams
      if (cmdContext.isCreateStream) {
        if (isCTAS) {
          // CSAS.
          // Get the query plan from SharkSemanticAnalyzer.
          // This is a DStream transform, so use TableRDDSinkOperator
          this.ctx = new QueryContext(conf, true)
          val newContext = new QueryContext(conf, true)
          newContext.setTryCount(cmdContext.getTryCount)
          newContext.setCmd(cmdContext.getCmd)
        } else {
          // TODO: Use a StreamDesc, parent CreateTableDesc?
          super.analyzeInternal(ast)
          // SemanticAnalyzer's td is null. Get it from DDLWork.
          val td = rootTasks.head.getWork.asInstanceOf[DDLWork].getCreateTblDesc
          analyzeCreateStream(td)
          return
        }
      } else {
        // Regular CREATE TABLE/CTAS
        super.analyzeInternal(ast)
        return
      }
    } else {
      // This is a query. Still need to check for table sources that are streams.
      SessionState.get().setCommandType(HiveOperation.QUERY)
      ASTTraversal.processQueryNode(ast, cmdContext)
    }
    
    // Generate Shark SparkTasks and get parse info.
    super.analyzeInternal(ast)
    pctx = getParseContext()

    // Determine if there is a stream source.
    if (cmdContext.streamToWindow.size == 0) {
      if (cmdContext.isCreateStream && isCTAS) {
        throw new SemanticException(
          "Must include at least one stream source for creating a derived stream")
      } else {
        return
      }
    }

    // At this point, the command is either a CSAS, a real-time query that involves
    // stream(s), or an "archive" command.

    // Get the tasks generated by SharkSemanticAnalyzer.
    // There's one SparkTask created for each TerminalOperator.
    val sparkTasks = rootTasks.asInstanceOf[JavaList[SparkTask]]

    // Find all stream source TableScanOperators, and convert them to StreamScanOperators.
    // TODO: The populate operator metadata related to streams is added here, but maybe
    // should be done during Task initialization.
    if (sparkTasks.size == 1) {
      // If the any TableScanOperator is for a DStream input source,
      // replace it with StreamScanOperator.
      val terminalOp = sparkTasks.head.getWork.terminalOperator
      val topToTable = pctx.getTopToTable
      val inputStreams = new ArrayBuffer[DStream[_]]()

      for (topOp <- terminalOp.returnTopOperators) {
        val tableName = topToTable.get(topOp.hiveOp).getTableName
        SharkEnv.streams.getStream(tableName) match {
          case stream: DStream[_] => {
            val streamScanOp = StreamingSemanticAnalyzer.convertTableScanToStreamScan(
              topOp.asInstanceOf[TableScanOperator])

            // Do some StreamScanOp initialization...move to CQTask?
            streamScanOp.tableName = tableName
            streamScanOp.windowDuration = cmdContext.streamToWindow.get(tableName)

            // TODO: should we set source stream for each StreamScanOp here?
            //       Depends on whether streams are immutable...
            inputStreams.append(stream)
            cmdContext.streamOps.append(streamScanOp)
          }
          case _ => Unit
        }
      }

      if (cmdContext.isCreateStream && isCTAS) {
        // If CSAS, add the transformedDStream to metadata
        val td = pctx.getQB.getTableDesc
        cmdContext.tableName = td.getTableName

        // TODO: pass this through cmdContext at parsing stage.
        val tblProps = td.getTblProps
        // Seconds
        val durationStr = tblProps.get("batch")
        if (durationStr != null) {
          cmdContext.duration = Duration(durationStr.toLong * 1000)
        }
        cmdContext.isDerivedStream = true

      } 

      // TODO: isArchiveStream should be set during AST traversal.
      if (pctx.getQB.getParseInfo.isInsertToTable && !qb.isCTAS) {
        cmdContext.isArchiveStream = true
      }

      genStreamingTasks(cmdContext, inputStreams, sparkTasks.head)

    } else {
      // Don't support mutiple SparkTasks created from condensed DDLs (ex. multi-insert).
      throw new SemanticException(
        "Can't do multiple SparkTask plan generation in streaming mode yet")
    }

    // ================
    // For debugging
    SharkEnv.streams.addCmdContext(cmdContext)
    // ================
    logInfo("Completed streaming plan generation")
  }

  // Create input stream for given table.
  def analyzeCreateStream(td: CreateTableDesc) {
    val tblProps = td.getTblProps()
    val batchDuration = tblProps.getOrElse("batch", "1").toLong
    val readDirectory = tblProps.get("path")

    // Stream name
    val tableName = td.getTableName
    // Use seconds for now
    val duration = Duration(batchDuration * 1000)
    // This creates the FileInputStream, and handles metadata additions.
    SharkEnv.streams.createTextFileStream(tableName, readDirectory, duration)
  }

  def genStreamingTasks(
    cmdContext: StreamingCommandContext,
    sourceDStreams: Seq[DStream[_]],
    sparkTask: SparkTask
  ) {
    
    rootTasks.clear()

    val executor = getExecutor(sourceDStreams, cmdContext.duration)
    // Create the CQTask
    val cqTask = TaskFactory.get(new CQWork(cmdContext, sparkTask, executor), conf)

    if (cmdContext.isDerivedStream) {
      // Tasks created by Shark:
      // SparkTask -> {MoveTask, DDLTask}
      // Discard the MoveTask. Replace SparkTask with CQTask.
      // New plan: CQTask -> DDLTask
      cqTask.addDependentTask(sparkTask.getChildTasks.get(1))
      val oldChildTasks = sparkTask.getChildTasks
      while (!oldChildTasks.isEmpty) {
        sparkTask.removeDependentTask(oldChildTasks.head)
      }
    } else if (cmdContext.isArchiveStream) {
      // If the StreamingContext used for this executor DStream hasn't been started, add a
      // StreamingLaunchTask as a dependency to the CQTask, which adds an output DStream (foreach).
      if (!SharkEnv.streams.hasSscStarted(executor)) {
        val ssc = SharkEnv.streams.getSsc(executor)
        val launchTask = TaskFactory.get(
            new StreamingLaunchWork(SharkEnv.streams.getSsc(executor)), conf)
        
        if (ssc == null) {
          assert(ssc != null)
        }
            
        SharkEnv.streams.addStartedSsc(ssc)
        cqTask.addDependentTask(launchTask)
      }
    }

    rootTasks.add(cqTask)
  }

  def getExecutor(sourceDStreams: Seq[DStream[_]], duration: Duration): DStream[_] = {
    // Use the DStream with smallest slideDuration.
    val sourceDStream = sourceDStreams.sortWith(_.slideDuration < _.slideDuration).head
    // If the user provides a batch duration and there are > 1 sources, trust that
    // it will be a valid duration (for now)
    if (duration == null) {
      return sourceDStream
    } else {
      sourceDStream.window(duration, duration)
    }
  }
}

object StreamingSemanticAnalyzer {

  // TODO: these should be in a StreamingOperatorFactory
  def convertTableScanToStreamScan(tableScanOp: TableScanOperator): StreamScanOperator = {
    val newOp = _newStreamingOperatorInstance(classOf[StreamScanOperator], tableScanOp)
      _replaceOpInTree(tableScanOp, newOp)
    return newOp.asInstanceOf[StreamScanOperator]
  }

  private def _replaceOpInTree(originalOp: Operator[_], newOp: Operator[_]) {
    for (childOp <- originalOp.childOperators) {
      val parents = childOp.parentOperators
      var i = 0
      while (i < parents.size) {
        if (parents(i) == originalOp) parents.remove(i)
        i += 1
      }
      childOp.addParent(newOp)
    }
    for (parentOp <- originalOp.parentOperators) {
      var children = parentOp.childOperators
      var i = 0
      while (i < children.size) {
        if (children(i) == originalOp) children.remove(i)
        i += 1
      }
      parentOp.addChild(newOp)
    }
  }

  private def _newStreamingOperatorInstance[T <: HiveOperator](
    cls: Class[_ <: Operator[T]], sharkOp: Operator[T]): Operator[_] = {
    val newOp = cls.newInstance()
    newOp.hiveOp = sharkOp.hiveOp.asInstanceOf[T]
    return newOp
  }
}
